Excellent â€” letâ€™s turn **Thinker-RAG** into a submission-ready repo that looks like it came from a senior AI systems engineer.

Below is a fully polished **`README.md`** you can drop into your repo root.
It includes:

* a clear executive summary,
* architecture diagram (ASCII + reference image),
* endpoint documentation with curl examples,
* setup instructions,
* design rationale for your stack,
* scalability notes,
* and a short section for your demo video link.

---

## ğŸ§  `README.md` (ready to copyâ€“paste)

````markdown
# ğŸ§  Thinker-RAG  
> Scalable, Asynchronous, Web-Aware Retrieval-Augmented Generation Engine  
> (AiRA Technical Challenge â€” System Design Submission)

---

## 1ï¸âƒ£ Overview

**Thinker-RAG** is a production-grade prototype of a *living, memory-driven RAG system* inspired by AiRAâ€™s mission:  
> _â€œAI systems that learn, remember, and evolve with users over time.â€_

This engine asynchronously ingests web content, converts it into vectorized knowledge using **Gemini embeddings**, and enables users to query it via **grounded, fact-based answers** generated by **Gemini LLMs**.

It is built to be:
- **Asynchronous:** ingestion runs in background workers via Redis Queue  
- **Scalable:** containerized services (FastAPI, Qdrant, Redis, Postgres)  
- **Web-aware:** fetches and cleans real web pages  
- **Memory-driven:** persistent vector and metadata stores  
- **Extensible:** pluggable embedding and generation providers (Gemini by default)

---

## 2ï¸âƒ£ System Architecture

### Logical Flow

```text
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Client /  â”‚  POST    â”‚   FastAPI     â”‚   Push   â”‚     Redis      â”‚
        â”‚  User App  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Ingestion   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Queue      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                  â”‚
                                                     Worker pulls â”‚
                                                                  â–¼
                                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                           â”‚  Worker    â”‚
                                                           â”‚ (RQ)       â”‚
                                                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                                           â”‚ Fetch URL  â”‚
                                                           â”‚ Clean HTML â”‚
                                                           â”‚ Chunk Text â”‚
                                                           â”‚ Embed via  â”‚
                                                           â”‚ Gemini API â”‚
                                                           â”‚ Upsert to  â”‚
                                                           â”‚ Qdrant     â”‚
                                                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                             â”‚                                     â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  Qdrant       â”‚â—€â”€â”€â”€â”€ Query â”€â”€â”€â”€â”€â–¶â”‚   FastAPI /    â”‚
                                   â”‚  Vector Store â”‚                   â”‚   /query API   â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚ Gemini LLM  â”‚
                                        â”‚ Answer Gen  â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
````

ğŸ“ **Component summary**

| Component            | Purpose                                    | Tech              |
| -------------------- | ------------------------------------------ | ----------------- |
| `api/`               | REST API (FastAPI) for ingestion + queries | Python + FastAPI  |
| `worker/`            | Asynchronous ingestion worker              | RQ + Redis        |
| `model/`             | Embedding + LLM provider (Gemini API)      | Google GenAI SDK  |
| `storage/`           | Vector + metadata persistence              | Qdrant + Postgres |
| `docker-compose.yml` | Orchestrates full stack                    | Docker            |

---

## 3ï¸âƒ£ API Documentation

### `POST /ingest-url`

Submit a web URL for asynchronous ingestion.

**Request**

```json
{ "url": "https://en.wikipedia.org/wiki/OpenAI" }
```

**Response**

```json
{ "job_id": "a34e1d2b-...", "status": "queued" }
```

---

### `GET /ingest-status/{job_id}`

Retrieve the ingestion status.

**Response**

```json
{ "job_id": "a34e1d2b-...", "status": "finished", "result": {"status": "completed", "chunks": 42} }
```

---

### `POST /query`

Ask a factual question grounded in the ingested knowledge base.

**Request**

```json
{ "query": "What is OpenAI?", "top_k": 5 }
```

**Response (example)**

```json
{
  "answer": "{ 'answer': 'OpenAI is an AI research lab...', 'sources': [...] }",
  "sources": [
    {"url":"https://en.wikipedia.org/wiki/OpenAI","chunk_index":0,"text":"OpenAI is a San Franciscoâ€“based..."}
  ]
}
```

---

## 4ï¸âƒ£ Quick Start

### Prerequisites

* Docker & Docker Compose
* Google Gemini API key (see [https://ai.google.dev/](https://ai.google.dev/))

### Run locally

```bash
# 1. Clone
git clone https://github.com/yourusername/thinker-rag.git
cd thinker-rag

# 2. Configure environment
cp .env.example .env
# Edit .env to include your GEMINI_API_KEY

# 3. Launch
docker compose up --build
```

### Smoke test

```bash
# Ingest
curl -X POST http://localhost:8000/ingest-url -H "Content-Type: application/json" \
  -d '{"url":"https://en.wikipedia.org/wiki/OpenAI"}'

# Query
curl -X POST http://localhost:8000/query -H "Content-Type: application/json" \
  -d '{"query":"What is OpenAI?"}'
```

---

## 5ï¸âƒ£ Design Choices & Justifications

| Decision              | Rationale                                                    |
| --------------------- | ------------------------------------------------------------ |
| **FastAPI**           | High-performance, async-ready, automatic OpenAPI docs        |
| **Redis + RQ**        | Minimal-ops, lightweight async job queue for ingestion       |
| **Qdrant**            | Vector DB with filtering, sharding, and strong performance   |
| **Gemini API**        | Unified embedding + generation; low latency; easy scaling    |
| **Postgres (future)** | Ideal metadata store for provenance, ingestion tracking      |
| **Docker Compose**    | Self-contained environment, easy portability                 |
| **Trafilatura**       | Reliable web text extraction respecting structure & encoding |

---

## 6ï¸âƒ£ Scalability & Reliability

| Concern                | Solution                                                         |
| ---------------------- | ---------------------------------------------------------------- |
| **Throughput**         | Horizontal worker scaling (`docker compose up --scale worker=4`) |
| **Backpressure**       | RQ queue depth monitoring + retry policies                       |
| **Vector Search Load** | Qdrant sharding; batch upserts                                   |
| **Latency**            | Batch embeddings; cache popular queries in Redis                 |
| **Observability**      | Structured logs + Prometheus hooks (future work)                 |
| **Cost**               | Configurable fallback to local sentence-transformer embeddings   |

---

## 7ï¸âƒ£ Data Model

| Table              | Fields                                                 | Purpose                  |
| ------------------ | ------------------------------------------------------ | ------------------------ |
| **ingestion_jobs** | job_id, url, status, timestamps                        | Async job tracking       |
| **documents**      | doc_id, url, checksum, word_count                      | Canonical document info  |
| **chunks**         | chunk_id, doc_id, chunk_index, text_snippet, vector_id | Granular retrieval units |

**Qdrant Payload Example**

```json
{
  "url": "https://en.wikipedia.org/wiki/OpenAI",
  "job_id": "uuid",
  "chunk_index": 0,
  "text": "OpenAI is an AI research company..."
}
```

---

## 8ï¸âƒ£ Prompt Engineering

Gemini system prompt used for grounded answers:

```text
You are a factual AI assistant. Use ONLY the given context to answer.
Return the final JSON with 'answer' and 'sources'.
```

Example dynamic prompt sent to Gemini:

```text
[CHUNK_0] https://en.wikipedia.org/wiki/OpenAI
OpenAI is a San Franciscoâ€“based...

QUESTION: What is OpenAI?
Return JSON only.
```

---

## 9ï¸âƒ£ Deployment Notes

* **Prod scaling:** Move to managed Redis & Qdrant clusters; add API Gateway.
* **Caching layer:** Store embeddings in Redis for frequent URLs.
* **Auth:** Add API-Key header for external access.
* **CI/CD:** Docker build â†’ GitHub Actions â†’ Cloud Run or ECS.

---

## ğŸ”Ÿ Performance Metrics (baseline)

| Metric             | Target                      |
| ------------------ | --------------------------- |
| Ingestion latency  | < 5s for 3k-token pages     |
| Query retrieval    | < 1s vector search          |
| LLM generation     | < 2s average (Gemini-Flash) |
| End-to-end latency | < 3.5s for typical query    |

---

## ğŸ” Future Enhancements

* Full ingestion provenance in Postgres
* Automatic summarization for long docs
* Domain allow-lists + robots.txt respect
* Streaming response with Gemini Realtime API
* Prometheus metrics + Grafana dashboard

---

## ğŸ¥ Demo Video

Upload a short video (5â€“8 minutes) showing:

1. `docker compose up --build`
2. Ingest a URL
3. Show worker logs processing
4. Run a query and display the grounded Gemini answer
5. Briefly explain architecture (use the diagram in `docs/architecture.png`)

ğŸ‘‰ **Demo Link:** *(to be added after recording)*

---

## ğŸ§© Repository Structure

```
thinker-rag/
â”œâ”€â”€ api/                # FastAPI endpoints
â”œâ”€â”€ model/              # Gemini embedding + LLM providers
â”œâ”€â”€ worker/             # Async ingestion workers
â”œâ”€â”€ storage/            # Qdrant vector DB helpers
â”œâ”€â”€ scripts/            # Example curl scripts
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ docker-compose.yml  # Orchestrates services
â”œâ”€â”€ Dockerfile          # App container build
â”œâ”€â”€ .env.example        # Env template
â””â”€â”€ README.md           # This file
```

---

## ğŸ§‘â€ğŸ’» Author

**Anurag Mishra**
AI & ML Engineer | Data Scientist
ğŸ“§ [anuragmishra02@gmail.com](mailto:anuragmishra02@gmail.com)
ğŸ”— [LinkedIn](https://www.linkedin.com/in/anuragmishra02/)â€ƒ[GitHub](https://github.com/OPanurag)

---

> â€œSystems that think are built by engineers who make them remember.â€
> â€” *Thinker-RAG, 2025*

````

---

```bash
git add .
git commit -m "Complete Thinker-RAG system for AiRA submission"
git push origin main
````
